{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Import PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: In Jupyter, print the version of PyTorch installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attributes(obj):\n",
    "    \"\"\"Prints the class attributes (str) of an object.\n",
    "\n",
    "    Args:\n",
    "        obj : Any object in Python\n",
    "    \"\"\"    \n",
    "    attributes = ', '.join(i for i in dir(obj) if not i.startswith('_'))\n",
    "    # source note: https://tinyurl.com/yyef5trl | stack overflow\n",
    "    print(attributes+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- describe the \"why\"\n",
    "- PyTorch vs. TensorFlow\n",
    "- describe installation\n",
    "- describe recommended pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(5)\n",
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Print a tensor of dimension 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `ndim` not found.\n"
     ]
    }
   ],
   "source": [
    "torch.tensor(5).ndim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(5).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        getset_descriptor\n",
       "\u001b[1;31mString form:\u001b[0m <attribute 'ndim' of 'torch._C._TensorBase' objects>\n",
       "\u001b[1;31mDocstring:\u001b[0m   Alias for :meth:`~Tensor.dim()`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.Tensor.ndim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(5).ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Print a tensor of dimension 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5]).ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is the dimension of the following tensor? \n",
    "```python\n",
    "torch.tensor([5, 5])\n",
    "```\n",
    "Also, return it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5, 5]).ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Print a tensor of dimension 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5],\n",
       "        [5, 5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[5, 5],\n",
    "              [5, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return the matrix dimensions of `x` using an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                  [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return the shape of `x` by using a function, not an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2],\n",
    "                  [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size() # works like np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Multiply `x` by 2 element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise multiplication by a scalar\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Multiply `x` and `y` element-wise with an operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2513, 0.1479],\n",
       "        [0.0831, 0.0087]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "\n",
    "# element-wise multiplication of tensors\n",
    "x * y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Add `x` and `y` element-wise with an operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0045, 0.9407],\n",
       "        [0.5766, 0.3435]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# element-wise addition of tensors\n",
    "x + y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Add `x` and `y` element-wise with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0045, 0.9407],\n",
       "        [0.5766, 0.3435]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Also element-wise addition of tensors\n",
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return a 1D tensor of zeros with length 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return a 2 by 3 zero matrix (as a tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What method lets you return a tensor filled with random numbers from a uniform distribution\n",
    "on the interval $[0, 1)$?\n",
    "\n",
    "`torch.rand`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return a tensor of shape (2, ) filled w/ random numbers $\\sim\\text{Uniform}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4301, 0.6362])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return a tensor of shape (2,2) filled w/ random numbers $\\sim\\text{Uniform}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4524, 0.9985],\n",
       "        [0.8980, 0.6122]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand([2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Return a 2 by 5 tensor, $y$, filled w/ random numbers $\\sim\\text{Uniform}(0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3044, 0.1350, 0.9358, 0.2429, 0.8446],\n",
       "        [0.6894, 0.8933, 0.2210, 0.3393, 0.0139]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand([2, 5])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "view(*shape) -> Tensor\n",
       "\n",
       "Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
       "different :attr:`shape`.\n",
       "\n",
       "The returned tensor shares the same data and must have the same number\n",
       "of elements, but may have a different size. For a tensor to be viewed, the new\n",
       "view size must be compatible with its original size and stride, i.e., each new\n",
       "view dimension must either be a subspace of an original dimension, or only span\n",
       "across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
       "contiguity-like condition that :math:`\\forall i = 0, \\dots, k-1`,\n",
       "\n",
       ".. math::\n",
       "\n",
       "  \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
       "\n",
       "Otherwise, :meth:`contiguous` needs to be called before the tensor can be\n",
       "viewed. See also: :meth:`reshape`, which returns a view if the shapes are\n",
       "compatible, and copies (equivalent to calling :meth:`contiguous`) otherwise.\n",
       "\n",
       "Args:\n",
       "    shape (torch.Size or int...): the desired size\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> x = torch.randn(4, 4)\n",
       "    >>> x.size()\n",
       "    torch.Size([4, 4])\n",
       "    >>> y = x.view(16)\n",
       "    >>> y.size()\n",
       "    torch.Size([16])\n",
       "    >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
       "    >>> z.size()\n",
       "    torch.Size([2, 8])\n",
       "\n",
       "    >>> a = torch.randn(1, 2, 3, 4)\n",
       "    >>> a.size()\n",
       "    torch.Size([1, 2, 3, 4])\n",
       "    >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
       "    >>> b.size()\n",
       "    torch.Size([1, 3, 2, 4])\n",
       "    >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
       "    >>> c.size()\n",
       "    torch.Size([1, 3, 2, 4])\n",
       "    >>> torch.equal(b, c)\n",
       "    False\n",
       "\u001b[1;31mType:\u001b[0m      method_descriptor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.Tensor.view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3044, 0.1350],\n",
       "        [0.9358, 0.2429],\n",
       "        [0.8446, 0.6894],\n",
       "        [0.8933, 0.2210],\n",
       "        [0.3393, 0.0139]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view([5, 2]) # similar to ndarray.reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.1919e-10, 6.4522e-07, 8.1723e+20, 1.7247e-07, 4.2111e-11])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0194e-38, 8.4490e-39, 1.0469e-38, 9.3674e-39],\n",
       "        [9.9184e-39, 8.7245e-39, 9.2755e-39, 8.9082e-39],\n",
       "        [9.9184e-39, 8.4490e-39, 9.6429e-39, 1.0653e-38],\n",
       "        [1.0469e-38, 4.2246e-39, 1.0378e-38, 9.6429e-39]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5338, 0.7411],\n",
       "        [0.2965, 0.0275]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7411, 0.0275])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0708, 0.7817, 0.1098, 0.4757],\n",
       "         [0.4302, 0.7755, 0.5509, 0.3893],\n",
       "         [0.4159, 0.6323, 0.6677, 0.5191],\n",
       "         [0.8978, 0.3408, 0.9920, 0.3433]]),\n",
       " tensor([[0.0708, 0.7817, 0.1098, 0.4757],\n",
       "         [0.4159, 0.6323, 0.6677, 0.5191]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "x, x[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.add_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd =  np.arange(5)\n",
    "tenzin = torch.from_numpy(nd)\n",
    "\n",
    "tenzin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_attributes(obj):\n",
    "    \"\"\"Prints the class attributes (str) of an object.\n",
    "\n",
    "    Args:\n",
    "        obj : Any object in Python\n",
    "    \"\"\"    \n",
    "    attributes = ', '.join(i for i in dir(obj) if not i.startswith('_'))\n",
    "    # source note: https://tinyurl.com/yyef5trl | stack overflow\n",
    "    print(attributes+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchSampler, ChainDataset, ConcatDataset, DataLoader, Dataset, DistributedSampler, IterableDataset, RandomSampler, Sampler, SequentialSampler, Subset, SubsetRandomSampler, TensorDataset, WeightedRandomSampler, dataloader, dataset, distributed, get_worker_info, random_split, sampler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "display_attributes(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiprocessing_context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "display_attributes(DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Check if $x$ is a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.is_tensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Check if $x$ is stored as a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(5,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.,  6., 10.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(2, 10, steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1000,  0.4642,  2.1544, 10.0000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logspace(-1, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 0, 3, 1, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random permutation\n",
    "torch.randperm(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "arange(start=0, end, step=1, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
       "\n",
       "Returns a 1-D tensor of size :math:`\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil`\n",
       "with values from the interval ``[start, end)`` taken with common difference\n",
       ":attr:`step` beginning from `start`.\n",
       "\n",
       "Note that non-integer :attr:`step` is subject to floating point rounding errors when\n",
       "comparing against :attr:`end`; to avoid inconsistency, we advise adding a small epsilon to :attr:`end`\n",
       "in such cases.\n",
       "\n",
       ".. math::\n",
       "    \\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}\n",
       "\n",
       "Args:\n",
       "    start (Number): the starting value for the set of points. Default: ``0``.\n",
       "    end (Number): the ending value for the set of points\n",
       "    step (Number): the gap between each pair of adjacent points. Default: ``1``.\n",
       "    out (Tensor, optional): the output tensor.\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`). If `dtype` is not given, infer the data type from the other input\n",
       "        arguments. If any of `start`, `end`, or `stop` are floating-point, the\n",
       "        `dtype` is inferred to be the default dtype, see\n",
       "        :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to\n",
       "        be `torch.int64`.\n",
       "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
       "        Default: ``torch.strided``.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.arange(5)\n",
       "    tensor([ 0,  1,  2,  3,  4])\n",
       "    >>> torch.arange(1, 4)\n",
       "    tensor([ 1,  2,  3])\n",
       "    >>> torch.arange(1, 2.5, 0.5)\n",
       "    tensor([ 1.0000,  1.5000,  2.0000])\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.arange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10, 20, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0217, -0.7316, -2.1996, -0.3324,  2.6727],\n",
       "        [-0.1591, -0.4618, -0.3964, -2.6516, -0.2281],\n",
       "        [ 0.8170,  0.1279, -2.3949,  1.2962, -1.0427],\n",
       "        [-1.9214, -1.2680,  1.4160, -0.9041,  1.7445]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "randint(low=0, high, size, *, generator=None, out=None, \\\n",
       "        dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
       "\n",
       "Returns a tensor filled with random integers generated uniformly\n",
       "between :attr:`low` (inclusive) and :attr:`high` (exclusive).\n",
       "\n",
       "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
       "\n",
       ".. note:\n",
       "    With the global dtype default (``torch.float32``), this function returns\n",
       "    a tensor with dtype ``torch.int64``.\n",
       "\n",
       "Args:\n",
       "    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\n",
       "    high (int): One above the highest integer to be drawn from the distribution.\n",
       "    size (tuple): a tuple defining the shape of the output tensor.\n",
       "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
       "    out (Tensor, optional): the output tensor.\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
       "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
       "        Default: ``torch.strided``.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.randint(3, 5, (3,))\n",
       "    tensor([4, 3, 4])\n",
       "\n",
       "\n",
       "    >>> torch.randint(10, (2, 2))\n",
       "    tensor([[0, 2],\n",
       "            [5, 5]])\n",
       "\n",
       "\n",
       "    >>> torch.randint(3, 10, (2, 2))\n",
       "    tensor([[4, 5],\n",
       "            [6, 7]])\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.randint?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 1, 1],\n",
       "        [1, 1, 3],\n",
       "        [0, 1, 4]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tenzin = torch.randint(high=5, size=(3,3))\n",
    "tenzin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: If we want to returns the indices of the minimum value of all elements in a tensor, we'd use `torch.argmin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(tenzin, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(tenzin, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.argmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0568, 0.6620, 0.6469, 0.6562]),\n",
       " tensor([0.5470, 0.9248, 0.2639, 0.0231]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = torch.rand(4), torch.rand(4)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0568, 0.6620, 0.6469, 0.6562, 0.5470, 0.9248, 0.2639, 0.0231])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([0, 1, 2]), tensor([0, 1, 2]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, z = torch.arange(3), torch.arange(3), torch.arange(3)\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 0, 1, 2, 0, 1, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, y, z), dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]),\n",
       " tensor([[2., 0., 0.],\n",
       "         [0., 2., 0.],\n",
       "         [0., 0., 2.]]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3)\n",
    "y = 2*torch.eye(3)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [2., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [0., 0., 2.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, y), dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 2., 0., 0.],\n",
       "        [0., 1., 0., 0., 2., 0.],\n",
       "        [0., 0., 1., 0., 0., 2.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x, y), dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.chunk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.eye(4)\n",
    "torch.chunk(I, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]),)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(I, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.]]),\n",
       " tensor([[0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 1., 0.]]),\n",
       " tensor([[0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(I, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(I, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.]]),\n",
       " tensor([[0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 1., 0.]]),\n",
       " tensor([[0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(I, 4)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.]]),\n",
       " tensor([[0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(input=I,chunks=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [1., 0.],\n",
       "         [0., 1.]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(input=I,chunks=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(4).reshape(2,2)\n",
    "torch.Tensor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.gather?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.nonzero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.eye(3)\n",
    "torch.nonzero(torch.Tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 3],\n",
       "        [1, 3, 0],\n",
       "        [2, 2, 1]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(4, (3,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [2, 0],\n",
       "        [2, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 3],\n",
       "        [1, 3, 0],\n",
       "        [2, 2, 1]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 3],\n",
       "         [1, 3, 0]]),\n",
       " tensor([[2, 2, 1]]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(x, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 3]]), tensor([[1, 3, 0]]), tensor([[2, 2, 1]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 3],\n",
       "         [1, 3, 0],\n",
       "         [2, 2, 1]]),)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(x, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "page 20 in PyTorch Cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "randint(low=0, high, size, *, generator=None, out=None, \\\n",
       "        dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
       "\n",
       "Returns a tensor filled with random integers generated uniformly\n",
       "between :attr:`low` (inclusive) and :attr:`high` (exclusive).\n",
       "\n",
       "The shape of the tensor is defined by the variable argument :attr:`size`.\n",
       "\n",
       ".. note:\n",
       "    With the global dtype default (``torch.float32``), this function returns\n",
       "    a tensor with dtype ``torch.int64``.\n",
       "\n",
       "Args:\n",
       "    low (int, optional): Lowest integer to be drawn from the distribution. Default: 0.\n",
       "    high (int): One above the highest integer to be drawn from the distribution.\n",
       "    size (tuple): a tuple defining the shape of the output tensor.\n",
       "    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
       "    out (Tensor, optional): the output tensor.\n",
       "    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
       "        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
       "    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
       "        Default: ``torch.strided``.\n",
       "    device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
       "        Default: if ``None``, uses the current device for the default tensor type\n",
       "        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
       "        for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
       "    requires_grad (bool, optional): If autograd should record operations on the\n",
       "        returned tensor. Default: ``False``.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> torch.randint(3, 5, (3,))\n",
       "    tensor([4, 3, 4])\n",
       "\n",
       "\n",
       "    >>> torch.randint(10, (2, 2))\n",
       "    tensor([[0, 2],\n",
       "            [5, 5]])\n",
       "\n",
       "\n",
       "    >>> torch.randint(3, 10, (2, 2))\n",
       "    tensor([[4, 5],\n",
       "            [6, 7]])\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.randint?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = np.randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(2)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# not in place\n",
    "x = torch.ones(1)\n",
    "\n",
    "x_i = x\n",
    "x = x + 1\n",
    "\n",
    "print(x, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.]) tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# in place\n",
    "x = torch.ones(1)\n",
    "\n",
    "x_i = x\n",
    "x += 1\n",
    "\n",
    "print(x, x_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1,7)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(2, -1) # -1 makes torch infer the missing dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.Tensor.view?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Tensor.view does not create a copy. \n",
    "\n",
    "Different views share the same data.\n",
    "\n",
    "A function `reshape` exists, but it creates a copy of the Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # Check if we cna use GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: GPU usage basics [Evann Courdier](https://youtu.be/pWrwyOsho5A?t=734)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32,\n",
       " tensor([[0.6994, 1.3480, 3.0409, 1.2484],\n",
       "         [3.6850, 0.0273, 3.1673, 2.1888]]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((2,4))\n",
    "Y = 4 * x\n",
    "Y.dtype, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6992, 1.3477, 3.0410, 1.2480],\n",
       "        [3.6855, 0.0273, 3.1680, 2.1895]], dtype=torch.float16)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 3, 1],\n",
       "        [3, 0, 3, 2]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.to(torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.Tensor` can switch to and from the `np.ndarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "randint(low, high=None, size=None, dtype=int)\n",
       "\n",
       "Return random integers from `low` (inclusive) to `high` (exclusive).\n",
       "\n",
       "Return random integers from the \"discrete uniform\" distribution of\n",
       "the specified dtype in the \"half-open\" interval [`low`, `high`). If\n",
       "`high` is None (the default), then results are from [0, `low`).\n",
       "\n",
       ".. note::\n",
       "    New code should use the ``integers`` method of a ``default_rng()``\n",
       "    instance instead; see `random-quick-start`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "low : int or array-like of ints\n",
       "    Lowest (signed) integers to be drawn from the distribution (unless\n",
       "    ``high=None``, in which case this parameter is one above the\n",
       "    *highest* such integer).\n",
       "high : int or array-like of ints, optional\n",
       "    If provided, one above the largest (signed) integer to be drawn\n",
       "    from the distribution (see above for behavior if ``high=None``).\n",
       "    If array-like, must contain integer values\n",
       "size : int or tuple of ints, optional\n",
       "    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
       "    ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
       "    single value is returned.\n",
       "dtype : dtype, optional\n",
       "    Desired dtype of the result. Byteorder must be native.\n",
       "    The default value is int.\n",
       "\n",
       "    .. versionadded:: 1.11.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "out : int or ndarray of ints\n",
       "    `size`-shaped array of random integers from the appropriate\n",
       "    distribution, or a single such random int if `size` not provided.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "random_integers : similar to `randint`, only for the closed\n",
       "    interval [`low`, `high`], and 1 is the lowest value if `high` is\n",
       "    omitted.\n",
       "Generator.integers: which should be used for new code.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> np.random.randint(2, size=10)\n",
       "array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0]) # random\n",
       ">>> np.random.randint(1, size=10)\n",
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
       "\n",
       "Generate a 2 x 4 array of ints between 0 and 4, inclusive:\n",
       "\n",
       ">>> np.random.randint(5, size=(2, 4))\n",
       "array([[4, 0, 2, 1], # random\n",
       "       [3, 2, 2, 0]])\n",
       "\n",
       "Generate a 1 x 3 array with 3 different upper bounds\n",
       "\n",
       ">>> np.random.randint(1, [3, 5, 10])\n",
       "array([2, 2, 9]) # random\n",
       "\n",
       "Generate a 1 by 3 array with 3 different lower bounds\n",
       "\n",
       ">>> np.random.randint([1, 5, 7], 10)\n",
       "array([9, 8, 7]) # random\n",
       "\n",
       "Generate a 2 by 4 array using broadcasting with dtype of uint8\n",
       "\n",
       ">>> np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)\n",
       "array([[ 8,  6,  9,  7], # random\n",
       "       [ 1, 16,  9, 12]], dtype=uint8)\n",
       "\u001b[1;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.randint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [4, 1]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(low=0, \n",
    "                      high=5, \n",
    "                      size=(2,2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [4, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [4, 1]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = y.numpy()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from [video](https://youtu.be/pWrwyOsho5A?t=995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Disecting  a [tutorial](https://youtu.be/i2yPxY2rOzs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data from torchvision.datasets is cheating since most of your time will be spent on preparing your dataset. However, this will make other concepts easier to learn for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "MNIST('/data') # This will give you the following error\n",
    "``` \n",
    "```\n",
    "RuntimeError: Dataset not found. You can use download=True to download it\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"./data\", train=True, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()])) \n",
    "    # For some reason, the data in torchvision.datasets doesn't come \n",
    "    # in tensor form. transforms.Compose([transforms.ToTensor()]) fixes that\n",
    "test = datasets.MNIST(\"./data\", train=False, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the `batch_size` parameter? \n",
    "\n",
    "Normally, datasets are so large that they may not fit on memory. We'll often train the neural networks in batches `batch_size` number of samples at a time. \n",
    "\n",
    "Using a higher batch size generally helps training time, but there is a sweet spot. The tutorial recommends somewhere between 8 and 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([5, 9, 8, 3, 1, 3, 4, 5, 8, 9])]\n"
     ]
    }
   ],
   "source": [
    "# prints `batch_size` number of input-output pairs\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for item in data:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `data` is a list of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][1] # label of the second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1].shape # tensor of the second image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this image is 28 by 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][1].view(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we want the dataset to be as balanced as possible so that the model doesn't train itself into a local minima of loss that it cannot get out of. Below is a scheme for checking how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary of target counts \n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "total = 0\n",
    "for data in trainset:\n",
    "    Xs, Ys = data \n",
    "    for Y in Ys:\n",
    "        counter_dict[int(Y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def counts_barplot():\n",
    "    x = list(counter_dict.keys())\n",
    "    y = list(counter_dict.values())\n",
    "    ax = sns.barplot(x, y)\n",
    "    ax.set(title='Dataset Balance Chart', \n",
    "           xlabel='Digit', ylabel='Count')\n",
    "    plt.show()\n",
    "\n",
    "counts_barplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.87\n",
      "1: 11.24\n",
      "2: 9.93\n",
      "3: 10.22\n",
      "4: 9.74\n",
      "5: 9.04\n",
      "6: 9.86\n",
      "7: 10.44\n",
      "8: 9.75\n",
      "9: 9.92\n"
     ]
    }
   ],
   "source": [
    "# print dataset balance percentages using f string\n",
    "for i in counter_dict:\n",
    "    proportion = counter_dict[i]/total*100\n",
    "    print(f'{i}: {proportion:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f-string tutorial: [reference](https://realpython.com/python-f-strings/#old-school-string-formatting-in-python)\n",
    "- How to change the number of digits in f-string expression: [reference](https://stackoverflow.com/questions/45310254/fixed-digits-after-decimal-with-f-strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network imports\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__() # initialize nn.Module\n",
    "        # fc1 -> first fully connected layer\n",
    "        # apply linear transformation on incoming data\n",
    "        self.fc1 = nn.Linear(in_features=28*28, \n",
    "                             out_features=64)\n",
    "        \"\"\" nn.Linear(in_features, out_features, bias=True)\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "        \"\"\"\n",
    "        # fc2 must take in 64\n",
    "        self.fc2 = nn.Linear(in_features=64, \n",
    "                             out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, \n",
    "                             out_features=64)\n",
    "        # 10 classes -> output layer should have 10 nodes \n",
    "        self.fc4 = nn.Linear(in_features=64, \n",
    "                             out_features=10)\n",
    "    \n",
    "    def forward(self, x): # defines the forward propagation\n",
    "        # relu is an activation function\n",
    "        x = F.relu(self.fc1(x)) # relu on first layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Output layer needs a multiclassifying transformation\n",
    "        # log softmax works for this\n",
    "        x = self.fc4(x) \n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inherit, you inherit the methods and attributes the other module (`nn.Module`), however the initialization does not run. If you want the parent module to initialize too, you run `super().__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll get an error from running\n",
    "`output = net(X)` :\n",
    "\n",
    "`RuntimeError: size mismatch, m1: [28 x 28], m2: [784 x 64]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1542, -2.4014, -2.1882, -2.3896, -2.3724, -2.3394, -2.3511, -2.3672,\n",
       "         -2.2721, -2.2268]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28 * 28)\n",
    "# pass data through the NN and get return\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2262, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimize with Adam algorithm.\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\"\"\" torch.optim.Adam?\n",
    "torch.optim.Adam(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    "    weight_decay=0,\n",
    "    amsgrad=False,)\n",
    "Args:\n",
    "    params (iterable): iterable of parameters to optimize or dicts defining\n",
    "        parameter groups\n",
    "    lr (float, optional): learning rate (default: 1e-3)\n",
    "    betas (Tuple[float, float], optional): coefficients used for computing\n",
    "\"\"\"\n",
    "\n",
    "n_epochs = 3 # num of full passes throuagh data\n",
    "for epoch in range(n_epochs):\n",
    "    for data in trainset:\n",
    "        # data is a batch w/ featurs and targets\n",
    "        X, Y = data\n",
    "        net.zero_grad() # Zero the gradient buffers of all params\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # Loss metric: nll -> negative log likelihood \n",
    "        loss = F.nll_loss(output, Y) \n",
    "        # Use nll_loss when data is scalar\n",
    "        # Use MSE when data is one-hot\n",
    "        \n",
    "        loss.backward() # backward\n",
    "        optimizer.step() # adjusts weights\n",
    "    print(loss) # should see loss decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is `loss.backward` doing?\n",
    "\n",
    "When you call `loss.backward()`, PyTorch computes the gradient of loss w.r.t all the parameters in loss that have `requires_grad = True` and store them in `parameter.grad` attribute for every parameter.\n",
    "\n",
    "Q: What is `optimizer.step` doing?\n",
    "\n",
    "`optimizer.step()` updates all the parameters based on `parameter.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# without calculating gradients:\n",
    "with torch.no_grad():\n",
    "    # for data vector in dataset\n",
    "    for data in testset:\n",
    "        # X, Y are the feature and target vectors\n",
    "        X, Y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == Y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f\"Accuracy: {(correct/total):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the X[0] image\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# Print prediction -> pass X[0] thru NN\n",
    "print(torch.argmax(net(X[0].view(-1, 28*28))[0]))\n",
    "# X[0].view(-1, 28*28) -> reshapes for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
