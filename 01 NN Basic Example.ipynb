{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My First PyTorch Neural Network \n",
    "\n",
    "### Disecting  a [tutorial](https://youtu.be/i2yPxY2rOzs) from [Sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you'll need both `torch.nn` and `torch.nn.functional` if you're working with NNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting data from torchvision.datasets is cheating since most of your time will be spent on preparing your dataset. However, this will make other concepts easier to learn for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import MNIST dataset\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "MNIST('/data') # This will give you the following error\n",
    "``` \n",
    "```\n",
    "RuntimeError: Dataset not found. You can use download=True to download it\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"./data\", train=True, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()])) \n",
    "    # For some reason, the data in torchvision.datasets doesn't come \n",
    "    # in tensor form. transforms.Compose([transforms.ToTensor()]) fixes that\n",
    "test = datasets.MNIST(\"./data\", train=False, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why the `batch_size` parameter? \n",
    "\n",
    "Normally, datasets are so large that they may not fit on memory. We'll often train the neural networks in batches, which each have `batch_size` number of samples. \n",
    "\n",
    "Using a higher batch size generally helps training time, but there is a sweet spot. [Sentdex](https://www.youtube.com/channel/UCfzlCWGWYyIQ0aLC5w48gBQ) recommends somewhere between 8 and 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([2, 4, 2, 8, 4, 7, 5, 3, 5, 5])]\n"
     ]
    }
   ],
   "source": [
    "# prints `batch_size` number of input-output pairs\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for item in data:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `data` is a list of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][1] # label of the second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][1].shape # tensor of the second image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this image is 28 by 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data[0][1].view(28, 28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we want the dataset to be as balanced as possible so that the model doesn't train itself into a local minima of loss that it cannot get out of. Below is a scheme for checking how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary of target counts \n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "total = 0\n",
    "for data in trainset:\n",
    "    Xs, Ys = data \n",
    "    for Y in Ys:\n",
    "        counter_dict[int(Y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fb0f894fcaaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcounts_barplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-fb0f894fcaaf>\u001b[0m in \u001b[0;36mcounts_barplot\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     ax.set(title='Dataset Balance Chart', \n\u001b[0;32m      6\u001b[0m            xlabel='Digit', ylabel='Count')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "def counts_barplot():\n",
    "    x = list(counter_dict.keys())\n",
    "    y = list(counter_dict.values())\n",
    "    ax = sns.barplot(x, y)\n",
    "    ax.set(title='Dataset Balance Chart', \n",
    "           xlabel='Digit', ylabel='Count')\n",
    "    plt.show()\n",
    "\n",
    "counts_barplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset balance percentages using f string\n",
    "for i in counter_dict:\n",
    "    proportion = counter_dict[i]/total*100\n",
    "    print(f'{i}: {proportion:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f-string tutorial: [reference](https://realpython.com/python-f-strings/#old-school-string-formatting-in-python)\n",
    "- How to change the number of digits in f-string expression: [reference](https://stackoverflow.com/questions/45310254/fixed-digits-after-decimal-with-f-strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__() # initialize nn.Module\n",
    "        # fc1 -> first fully connected layer\n",
    "        # apply linear transformation on incoming data\n",
    "        self.fc1 = nn.Linear(in_features=28*28, \n",
    "                             out_features=64)\n",
    "        \"\"\" nn.Linear(in_features, out_features, bias=True)\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "        \"\"\"\n",
    "        # fc2 must take in 64\n",
    "        self.fc2 = nn.Linear(in_features=64, \n",
    "                             out_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, \n",
    "                             out_features=64)\n",
    "        # 10 classes -> output layer should have 10 nodes \n",
    "        self.fc4 = nn.Linear(in_features=64, \n",
    "                             out_features=10)\n",
    "    \n",
    "    def forward(self, x): # defines the forward propagation\n",
    "        # relu is an activation function\n",
    "        x = F.relu(self.fc1(x)) # relu on first layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # Output layer needs a multiclassifying transformation\n",
    "        # log softmax works for this\n",
    "        x = self.fc4(x) \n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you inherit, you inherit the methods and attributes the other module (`nn.Module`), however the initialization does not run. If you want the parent module to initialize too, you run `super().__init__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll get an error from running\n",
    "`output = net(X)` :\n",
    "\n",
    "`RuntimeError: size mismatch, m1: [28 x 28], m2: [784 x 64]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1542, -2.4014, -2.1882, -2.3896, -2.3724, -2.3394, -2.3511, -2.3672,\n",
       "         -2.2721, -2.2268]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28 * 28)\n",
    "# pass data through the NN and get return\n",
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0832, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2262, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# optimize with Adam algorithm.\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "\"\"\" torch.optim.Adam?\n",
    "torch.optim.Adam(\n",
    "    params,\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    "    weight_decay=0,\n",
    "    amsgrad=False,)\n",
    "Args:\n",
    "    params (iterable): iterable of parameters to optimize or dicts defining\n",
    "        parameter groups\n",
    "    lr (float, optional): learning rate (default: 1e-3)\n",
    "    betas (Tuple[float, float], optional): coefficients used for computing\n",
    "\"\"\"\n",
    "\n",
    "n_epochs = 3 # num of full passes throuagh data\n",
    "for epoch in range(n_epochs):\n",
    "    for data in trainset:\n",
    "        # data is a batch w/ featurs and targets\n",
    "        X, Y = data\n",
    "        net.zero_grad() # Zero the gradient buffers of all params\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # Loss metric: nll -> negative log likelihood \n",
    "        loss = F.nll_loss(output, Y) \n",
    "        # Use nll_loss when data is scalar\n",
    "        # Use MSE when data is one-hot\n",
    "        \n",
    "        loss.backward() # backward\n",
    "        optimizer.step() # adjusts weights\n",
    "    print(loss) # should see loss decreasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What is `loss.backward` doing?\n",
    "\n",
    "When you call `loss.backward()`, PyTorch computes the gradient of loss w.r.t all the parameters in loss that have `requires_grad = True` and store them in `parameter.grad` attribute for every parameter.\n",
    "\n",
    "Q: What is `optimizer.step` doing?\n",
    "\n",
    "`optimizer.step()` updates all the parameters based on `parameter.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# without calculating gradients:\n",
    "with torch.no_grad():\n",
    "    # for data vector in dataset\n",
    "    for data in testset:\n",
    "        # X, Y are the feature and target vectors\n",
    "        X, Y = data\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == Y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f\"Accuracy: {(correct/total):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the X[0] image\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# Print prediction -> pass X[0] thru NN\n",
    "print(torch.argmax(net(X[0].view(-1, 28*28))[0]))\n",
    "# X[0].view(-1, 28*28) -> reshapes for NN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
