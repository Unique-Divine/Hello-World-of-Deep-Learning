{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Data-Loading\" data-toc-modified-id=\"Imports-and-Data-Loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Data Loading</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataLoader-class\" data-toc-modified-id=\"DataLoader-class-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><a href=\"https://pytorch.org/docs/stable/data.html\" target=\"_blank\">DataLoader class</a></a></span></li></ul></li><li><span><a href=\"#Building-a-Neural--Network\" data-toc-modified-id=\"Building-a-Neural--Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Building a Neural  Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Activation Functions</a></span></li></ul></li><li><span><a href=\"#Training-and-Predictions\" data-toc-modified-id=\"Training-and-Predictions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training and Predictions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Loss Function</a></span></li><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#GPU\" data-toc-modified-id=\"GPU-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>GPU</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Training Loop</a></span></li></ul></li><li><span><a href=\"#Saving-and-Loading-the-Model\" data-toc-modified-id=\"Saving-and-Loading-the-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Saving and Loading the Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Saving-model-weights\" data-toc-modified-id=\"Saving-model-weights-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Saving model weights</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#Next-Steps:\" data-toc-modified-id=\"Next-Steps:-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Next Steps:</a></span><ul class=\"toc-item\"><li><span><a href=\"#TODO\" data-toc-modified-id=\"TODO-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>TODO</a></span></li><li><span><a href=\"#Feedback\" data-toc-modified-id=\"Feedback-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Feedback</a></span></li><li><span><a href=\"#Convolutional-NN-|-(Computer-Vision)-Cat-Dog\" data-toc-modified-id=\"Convolutional-NN-|-(Computer-Vision)-Cat-Dog-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Convolutional NN | (Computer Vision) Cat Dog</a></span></li><li><span><a href=\"#Recurrent-NN-|-Time-series-or-NLP-Tutorial\" data-toc-modified-id=\"Recurrent-NN-|-Time-series-or-NLP-Tutorial-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Recurrent NN | Time series or NLP Tutorial</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# standard DS stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# computer vision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# dataset loading\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "# convenient package for plotting loss functions\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import copy\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the focus of this tutorial is to understand some foundations about PyTorch and neural networks, we'll be using a small subset of a [dataset](#s0) suitable for a multivariate regression task. \n",
    "\n",
    "Dataset source note: book Comment Volume Dataset Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shapes\n",
      "Training set:(40949, 54), Testing set:(10044, 54)\n",
      "Dataset shapes after PCA and random sampling\n",
      "X_train.shape:(10237, 10), Y_train.shape:(10237,)\n",
      "X_test.shape:(5022, 10), Y_test.shape:(5022,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10237, 10), (10237,), (5022, 10), (5022,)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec(open(\"datasets/FB_comments/process_data.py\").read())\n",
    "[A.shape for A in [X_train, Y_train, X_test, Y_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DataLoader class](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "Every dataset, no matter whether what it includes, can interact with PyTorch if it satisfies the following abstract Python class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class arbitrary_Dataset(object):\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Retrieve an item from the dataset in a (label, tensor) pair.\n",
    "        \n",
    "        Args:\n",
    "            idx: index\n",
    "        \"\"\"\n",
    "        raise NeedsImplementationError\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the size of the dataset (len)\"\"\"\n",
    "        raise NeedsImplementationError\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is referred to as a map-style dataset in the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB_Dataset(Dataset): # inherit from torch's Dataset class.\n",
    "    def __init__(self, train):\n",
    "        # data loading\n",
    "        if train == True:\n",
    "            self.X = torch.from_numpy(X_train.astype(np.float32))\n",
    "            self.Y = torch.from_numpy(Y_train.reshape(-1,1).astype(np.float32))\n",
    "        else:\n",
    "            self.X = torch.from_numpy(X_test.astype(np.float32))\n",
    "            self.Y = torch.from_numpy(Y_test.reshape(-1,1).astype(np.float32))\n",
    "\n",
    "        if self.X.shape[0] == self.Y.shape[0]:\n",
    "            self.n_samples = self.X.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        # len(dataset)\n",
    "\n",
    "train_set = FB_Dataset(train=True)\n",
    "test_set = FB_Dataset(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_dl = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These torch `DataLoader` objects behave just like MNIST ones from the previous tutorial. \n",
    "\n",
    "Just as a reminder, I'll print the tensor shapes of a single batch from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10]) torch.Size([100, 1])\n",
      "Shape of single feature: torch.Size([10])\n",
      "Shape of single target: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for data in train_dl:\n",
    "    features, labels = data\n",
    "    print(features.shape, labels.shape)\n",
    "    break\n",
    "print(f\"Shape of single feature: {data[0][0].shape}\") # 1D tensor\n",
    "print(f\"Shape of single target: {data[1][0].shape}\") # 0D tensor (scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = torch.flatten(data[0][0]).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural  Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=7, bias=True)\n",
      "  (fc2): Linear(in_features=7, out_features=3, bias=True)\n",
      "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # initialize nn.Module\n",
    "        # some fully connected layers w/ linear transformation\n",
    "        \"\"\" nn.Linear(in_features, out_features, bias=True)\n",
    "        Args:\n",
    "            in_features: size of each input sample. For input shape (28, 28), \n",
    "                we would have in_features = 28 * 28 = 784\n",
    "            out_features: size of each output sample.\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(n_features, 7)\n",
    "        self.fc2 = nn.Linear(7, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "    def forward(self, x): # defines the forward propagation\n",
    "        x = F.relu(self.fc1(x)) # relu activation function\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        # Output layer needs a multiclassifying transformation\n",
    "        # log softmax works for this\n",
    "        return x\n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above network is known as a **feedforward neural network** or **multilayer perceptron (MLP)** ([Goodfellow et al., Deep Learning Book](#s1)). It's called feedforward because there are no **feedback** connections in which the outputs of the model are fed back into previous layers.  \n",
    "\n",
    "When feedforward neural networks are extended to include feedback connections, they are called **recurrent neural netowrks**. \n",
    "\n",
    "The **depth** of a network is defined as its number of layers (including the output layer but excluding input layer), while the **width** of a network is defined to be the maximal number of nodes in a layer. This explains the reasoning behind the name, \"deep learning\".\n",
    "\n",
    "The terminology for the network structure above is typical called the **network architecture**, which includes how many layers the network contains, how the layers are connected to each other, and how many neurons (a.k.a nodes, a.k.a. units) are in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions are used to compute hidden layer values.\n",
    "\n",
    "Here, we make use of the **rectified linear unit (ReLU)** as the activation function, using `F.relu`. This is the default recommendation for the activation function in modern deep learning. Here's [an article](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) about ReLU for your reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a network involves passing data through the network, using the **loss function** to ~~determine~~ define a criterion for capturing the similarity or difference between a prediction and an actual target.\n",
    "\n",
    "Below I'll include common loss functions for various supervised learning tasks:\n",
    "\n",
    "**Regression**:\n",
    "- Mean squared error: [`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "- Mean absolute error, or L1: [`nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss)\n",
    "\n",
    "**Binary Classification**:\n",
    "- Binary cross-entropy: [`nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)\n",
    "- Binary cross-entropy with logits: [`nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss) \n",
    "\n",
    "**Multi-class Classification**:\n",
    "- Cross entropy : [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "- Negative log likelihood: [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss() # ex. regression\n",
    "# loss_fn = nn.BCELoss() # ex. binary classification\n",
    "# loss_fn = nn.CrossEntropyLoss() # ex. multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "The information about the loss function that is gained when we pass data through the network is used to update the weights of the network such that we  minimize the loss function.\n",
    "\n",
    "In order to perform the updates on the neural network, we use an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Adam optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **learning rate**, `lr`, is often a key parameter that needs to be tweaked in order to get a network to learn properly and efficiently.\n",
    "\n",
    "Adaptive moment estimation (Adam) and stochastic gradient descent (SGD) have been empirically shown to outperform most other optimizers in deep learning networks. \n",
    "\n",
    "I decided to use Adam for this tutorial because it, along with RMSProp and AdaGrad, uses an adaptive learning rate, which adapts its updates to each paramter depending on the importance of individual paramters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "PyTorch, by default, does CPU-based calculations. To take advantage of the GPU, the input tensors and model need to be moved to the GPU explicitly with the `to()` method.\n",
    "\n",
    "`network` is simply an instance of the neural network class written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Recipe:\n",
    "if torch.cuda.is_available(): # If PyTorch reports that GPU is available\n",
    "    device = torch.device(\"cuda\") # device = GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # device = CPU\n",
    "\n",
    "network = network.to(device) # Copy model to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "First, some terminology. \n",
    "\n",
    "An **epoch** is a single forward and backward pass through ALL of the samples.\n",
    "\n",
    "A **batch**, then, refers to some subset of an the total dataset, where `batch_size` is the number of samples used in one forward and backward pass.\n",
    "\n",
    "The **number of iterations** is the number of passes (forward and backward $\\implies$ 1 pass) needed to complete a single epoch with each pass using `batch_size` number of samples.\n",
    "\n",
    "In other words, suppose that we have 50,000 samples and `batch_size=25`, then there are 50,000/25 == 2000 iterations for 1 epoch.\n",
    "\n",
    "TODO: Explain backpropagation at high level.\n",
    "\n",
    "TODO: Add practical explanation to code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, loss_fn, train_loader, val_loader,\n",
    "          n_epochs, optimizer=optimizer, device=device):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        network (nn.Module-like): NN object\n",
    "        loss_fn: loss function\n",
    "        train_loader (DataLoader): PyTorch DataLoader for training set\n",
    "        val_loader (DataLoader): PyTorch DataLoader for validation set\n",
    "        n_epochs (int): number of epochs := # of full passes through data\n",
    "        optimizer (torch.optim optimizer): SGD, Adam, etc.\n",
    "        device (torch.device, optional): GPU selection. OR, uses CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training\n",
    "        network.train()\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # clears gradient buffers of all parameters\n",
    "            inputs, targets = batch\n",
    "            # transfer batch data to computation device\n",
    "            [inputs, targets] = [tensor.to(device) for tensor in [inputs, targets]]\n",
    "            output = network(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step() # update model weights\n",
    "            train_loss += loss.data.item()\n",
    "#         train_loss /= len(train_iterator)\n",
    "        train_losses.append(train_loss)\n",
    "    \n",
    "        if idx % 10 == 0:\n",
    "            print(f\"epoch {epoch+1}/{n_epochs}, batch {idx}.\")\n",
    "        \n",
    "        # Validation \n",
    "        network.eval()        \n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            [inputs, targets] = [tensor.to(device) for tensor in batch]\n",
    "            output = network(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            val_loss += loss.data.item()\n",
    "#         val_loss /= len(valid_iterator)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "        print(f\"Epoch: {epoch}, Training Loss: {train_loss:.3f}, \"\n",
    "             +f\"Validation loss: {val_loss:.3f}\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.tight_layout()\n",
    "    ax.plot(np.arange(n_epochs), train_losses, '-')\n",
    "    ax.plot(np.arange(n_epochs), val_losses, '-')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "#     return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 889921.911, Validation loss: 926878.614\n",
      "Epoch: 1, Training Loss: 1124810.404, Validation loss: 722125.621\n",
      "Epoch: 2, Training Loss: 236497.161, Validation loss: 680841.566\n",
      "Epoch: 3, Training Loss: 237740.872, Validation loss: 707724.129\n",
      "Epoch: 4, Training Loss: 213483.938, Validation loss: 686630.758\n",
      "Epoch: 5, Training Loss: 176932.320, Validation loss: 676024.332\n",
      "Epoch: 6, Training Loss: 168194.871, Validation loss: 688733.690\n",
      "Epoch: 7, Training Loss: 163164.987, Validation loss: 671650.364\n",
      "Epoch: 8, Training Loss: 161486.506, Validation loss: 668807.558\n",
      "Epoch: 9, Training Loss: 167017.958, Validation loss: 674736.409\n",
      "Epoch: 10, Training Loss: 163809.086, Validation loss: 670517.963\n",
      "Epoch: 11, Training Loss: 160946.706, Validation loss: 673208.125\n",
      "Epoch: 12, Training Loss: 162136.741, Validation loss: 666734.928\n",
      "Epoch: 13, Training Loss: 165861.708, Validation loss: 676203.259\n",
      "Epoch: 14, Training Loss: 171811.590, Validation loss: 742375.762\n",
      "Epoch: 15, Training Loss: 162868.913, Validation loss: 689061.247\n",
      "Epoch: 16, Training Loss: 170712.084, Validation loss: 664703.984\n",
      "Epoch: 17, Training Loss: 160407.448, Validation loss: 664602.683\n",
      "Epoch: 18, Training Loss: 158379.822, Validation loss: 659693.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss: 160031.745, Validation loss: 659043.807\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(network=network, loss_fn=loss_fn, optimizer=optimizer,\n",
    "      train_loader=train_dl, val_loader=test_dl, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the whole network**:\n",
    "\n",
    "To save the current state of the model in [pickle](https://docs.python.org/3/library/pickle.html) format, including the architecture and model weights, use `torch.save()`. This can be loaded at a later time with `torch.load()`. For example,\n",
    "```python\n",
    "torch.save(network, \"temp/network\")\n",
    "# ... at a later time\n",
    "network = torch.load(\"temp/network\")\n",
    "```\n",
    "- *Note*, saving the architecture and the weights of a model can be problematic if you decide to change the structure of in the future. A better approach would be...\n",
    "\n",
    "### Saving model weights\n",
    "\n",
    "- Save the model's (here, that's `network`) `state_dict`, a dictionary that can map to the parameters (weights) in each layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_path = r\"C:/Users/uniqu/Adaptation/\"\n",
    "local_path = \"temp/saved_weights\"\n",
    "path = system_path + local_path\n",
    "torch.save(network.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the above file paths will be different for you. I am pleb that uses Windows, so the function throws errors if I don't use the full path instead of the local one. If you need to find out what directory you're in, type `!cd` \n",
    "into a jupyter cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-1.0380e-01, -1.0236e-01, -2.1321e-01,  6.1140e-02,  1.3036e-01,\n",
       "                       -1.5332e-01,  4.3006e-01, -9.6006e-03, -3.2798e-01, -8.9341e-01],\n",
       "                      [ 1.7195e-01,  1.9770e-01,  3.2613e-01,  1.4483e-01, -2.3777e-01,\n",
       "                       -5.3883e-01, -2.3455e-01,  4.7106e-02, -1.7805e-01, -4.3461e-01],\n",
       "                      [-1.3407e-01, -1.9913e-01,  2.4303e-02, -1.0618e-01, -2.0531e-01,\n",
       "                       -2.0362e-01,  4.9991e-01, -2.1486e-02, -3.8847e-01, -4.7605e-01],\n",
       "                      [ 6.0997e-02, -2.9213e-01,  4.5105e-01,  2.1478e-01, -7.6541e-02,\n",
       "                       -1.5227e-01,  2.2980e-04,  5.6312e-02,  3.3698e-01, -2.3694e-01],\n",
       "                      [ 6.3868e-02, -4.9727e-02, -2.8943e-02, -1.6753e-01,  3.8951e-01,\n",
       "                        7.4371e-02, -1.8528e-01, -1.8068e-01, -3.7331e-01,  1.2248e-01],\n",
       "                      [-1.0320e-01, -1.9923e-01,  5.2859e-03,  2.1324e-02,  1.6801e-01,\n",
       "                       -5.0826e-02,  1.0252e-01, -2.0811e-01,  3.1458e-01,  3.8854e-01],\n",
       "                      [-1.4294e-01, -9.1683e-02,  6.2457e-02, -1.8836e-02,  1.1216e-01,\n",
       "                        2.8080e-01, -2.0703e-01,  1.1001e-01, -7.4593e-02,  1.2018e-01]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.0277,  0.5148,  0.0615,  0.0517, -0.0161, -0.0027, -0.1961])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.2156, -0.9360,  0.0142,  0.0123,  0.1734, -0.3722,  0.1943],\n",
       "                      [ 0.0690, -0.3912, -0.0305, -0.4540,  0.2811, -0.1213, -0.0957],\n",
       "                      [-0.0742, -0.2633, -0.2796, -0.2371,  0.0486,  0.0763,  0.0930]])),\n",
       "             ('fc2.bias', tensor([ 0.2800,  0.0932, -0.0669])),\n",
       "             ('fc3.weight', tensor([[-0.0151, -0.0042,  0.0706]])),\n",
       "             ('fc3.bias', tensor([6.8899]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... at a later time\n",
    "network_state_dict = torch.load(path); network_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=7, bias=True)\n",
      "  (fc2): Linear(in_features=7, out_features=3, bias=True)\n",
      "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "new_net = Net(); print(new_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_net.load_state_dict(network_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One large benefit here is that the you can set the argument `strict = False` in `load_state_dict()` to have the function ignore non-matching keys, which enables you to assign parameters only to layers that *do* exist in the state dictionary and **prevents the load from failing even if the new architecture doesn't match the old one**.\n",
    " \n",
    "This means that you can **load in and use parameters from an entirely different model**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- <a id='s0'> </a> UCI Machine Learning Repository. *Facebook Comment Volume Dataset*. https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset# \n",
    "- <a id='s1'></a>Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep learning* (Vol. 1). Cambridge: MIT press.\n",
    "- Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). *The expressive power of neural networks: A view from the width*. In Advances in neural information processing systems (pp. 6231-6239).\n",
    "- Brownlee, J. (2019). A gentle introduction to the rectified linear unit (relu). *Machine Learning Mastery. https://machinelearningmastery.com/rectified-linear-activation-function-fordeep-learning-neural-networks*.\n",
    "- Inkawhich, M. (2017). [*Saving and Loading Models*](https://pytorch.org/tutorials/beginner/saving_loading_models.html#:~:text=save()%20to%20serialize%20the,the%20dictionary%20locally%20using%20torch) . PyTorch Official Docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Next Steps:\n",
    "\n",
    "### TODO\n",
    "- Cross validation\n",
    "- Hyperparameter Optimization\n",
    "- Consider benchmarking against other models (random forest, XGBoost)\n",
    "- Explain how batch size affects the number of computations\n",
    "\n",
    "### Feedback\n",
    "- Have some beginners in DL go through this tutorial and give feedback.\n",
    "\n",
    "### Convolutional NN | (Computer Vision) Cat Dog\n",
    "  - Save a CNN and use it on this dataset. \n",
    "  - Explain fundamental CNN concepts. \n",
    "  - Data from [Kaggle competition](https://www.kaggle.com/c/dogs-vs-cats/overview)\n",
    "  - [jaeboklee](https://www.kaggle.com/jaeboklee/pytorch-cat-vs-dog)\n",
    "\n",
    "### Recurrent NN | Time series or NLP Tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
