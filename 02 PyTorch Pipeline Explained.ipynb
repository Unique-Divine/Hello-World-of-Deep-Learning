{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Data-Loading\" data-toc-modified-id=\"Imports-and-Data-Loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Data Loading</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataLoader-class\" data-toc-modified-id=\"DataLoader-class-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><a href=\"https://pytorch.org/docs/stable/data.html\" target=\"_blank\">DataLoader class</a></a></span></li></ul></li><li><span><a href=\"#Building-a-Neural--Network\" data-toc-modified-id=\"Building-a-Neural--Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Building a Neural  Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Activation Functions</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Loss Function</a></span></li><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#GPU\" data-toc-modified-id=\"GPU-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>GPU</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Training Loop</a></span></li></ul></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Predictions</a></span></li><li><span><a href=\"#Saving-and-Loading-the-Model\" data-toc-modified-id=\"Saving-and-Loading-the-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Saving and Loading the Model</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#Next-Steps:\" data-toc-modified-id=\"Next-Steps:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Next Steps:</a></span><ul class=\"toc-item\"><li><span><a href=\"#(Computer-Vision)-Cat-Dog\" data-toc-modified-id=\"(Computer-Vision)-Cat-Dog-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>(Computer Vision) Cat Dog</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# standard DS stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# computer vision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# dataset loading\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "import copy\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the focus of this tutorial is to understand some foundations about PyTorch and neural networks, we'll be using a small subset of a [dataset](#s0) suitable for a multivariate regression task. \n",
    "\n",
    "Dataset source note: book Comment Volume Dataset Data Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shapes\n",
      "Training set:(40949, 54), Testing set:(10044, 54)\n",
      "Dataset shapes after PCA and random sampling\n",
      "X_train.shape:(10237, 10), Y_train.shape:(10237,)\n",
      "X_test.shape:(5022, 10), Y_test.shape:(5022,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(10237, 10), (10237,), (5022, 10), (5022,)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec(open(\"datasets/FB_comments/process_data.py\").read())\n",
    "[A.shape for A in [X_train, Y_train, X_test, Y_test]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DataLoader class](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "Every dataset, no matter whether what it includes, can interact with PyTorch if it satisfies the following abstract Python class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Retrieve an item from the dataset in a (label, tensor) pair.\n",
    "        \n",
    "        Args:\n",
    "            idx: index\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the size of the dataset (len)\"\"\"\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is referred to as a map-style dataset in the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FB_Dataset(Dataset): # inherit from torch's Dataset class.\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        self.X = torch.from_numpy(np.vstack([X_train, X_test]))\n",
    "        self.Y = torch.from_numpy(np.concatenate([Y_train, Y_test]))\n",
    "\n",
    "        if self.X.shape[0] == self.Y.shape[0]:\n",
    "            self.n_samples = self.X.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"Shape mismatch\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "        # len(dataset)\n",
    "\n",
    "fb_dataset = FB_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=fb_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural  Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # initialize nn.Module\n",
    "        # some fully connected layers w/ linear transformation\n",
    "        \"\"\" nn.Linear(in_features, out_features, bias=True)\n",
    "        Args:\n",
    "            in_features: size of each input sample. For input shape (28, 28), \n",
    "                we would have in_features = 28 * 28 = 784\n",
    "            out_features: size of each output sample.\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "    def forward(self, x): # defines the forward propagation\n",
    "        x = F.relu(self.fc1(x)) # relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        # Output layer needs a multiclassifying transformation\n",
    "        # log softmax works for this\n",
    "        return F.log_softmax(x, dim=1)\n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above network is known as a **feedforward neural network** or **multilayer perceptron (MLP)** ([Goodfellow et al., Deep Learning Book](#s1)). It's called feedforward because there are no **feedback** connections in which the outputs of the model are fed back into previous layers.  \n",
    "\n",
    "When feedforward neural networks are extended to include feedback connections, they are called **recurrent neural netowrks**. \n",
    "\n",
    "The **depth** of a network is defined as its number of layers (including the output layer but excluding input layer), while the **width** of a network is defined to be the maximal number of nodes in a layer. This explains the reasoning behind the name, \"deep learning\".\n",
    "\n",
    "The terminology for the network structure above is typical called the **network architecture**, which includes how many layers the network contains, how the layers are connected to each other, and how many neurons (a.k.a nodes, a.k.a. units) are in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions are used to compute hidden layer values.\n",
    "\n",
    "Here, we make use of the **rectified linear unit (ReLU)** as the activation function, using `F.relu`. This is the default recommendation for the activation function in modern deep learning. Here's [an article](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) about ReLU for your reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a network involves passing data through the network, using the **loss function** to ~~determine~~ define a criterion for capturing the similarity or difference between a prediction and an actual target.\n",
    "\n",
    "Below I'll include common loss functions for various supervised learning tasks:\n",
    "\n",
    "**Regression**:\n",
    "- Mean squared error: [`nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "- Mean absolute error, or L1: [`nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss)\n",
    "\n",
    "**Binary Classification**:\n",
    "- Binary cross-entropy: [`nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html#torch.nn.BCELoss)\n",
    "- Binary cross-entropy with logits: [`nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss) \n",
    "\n",
    "**Multi-class Classification**:\n",
    "- Cross entropy : [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "- Negative log likelihood: [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss() # ex. regression\n",
    "# loss_fn = nn.BCELoss() # ex. binary classification\n",
    "# loss_fn = nn.CrossEntropyLoss() # ex. multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "The information about the loss function that is gained when we pass data through the network is used to update the weights of the network such that we  minimize the loss function.\n",
    "\n",
    "In order to perform the updates on the neural network, we use an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Adam optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement loss function\n",
    "\n",
    "TODO: Explain choice of loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **learning rate**, `lr`, is often a key parameter that needs to be tweaked in order to get a network to learn properly and efficiently.\n",
    "\n",
    "Adaptive moment estimation (Adam) and stochastic gradient descent (SGD) have been empirically shown to outperform most other optimizers in deep learning networks. \n",
    "\n",
    "I decided to use Adam for this tutorial because it, along with RMSProp and AdaGrad, uses an adaptive learning rate, which adapts its updates to each paramter depending on the importance of individual paramters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "PyTorch, by default, does CPU-based calculations. To take advantage of the GPU, the input tensors and model need to be moved to the GPU explicitly with the `to()` method.\n",
    "\n",
    "`network` is simply an instance of the neural network class written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU Recipe:\n",
    "if torch.cuda.is_available(): # If PyTorch reports that GPU is available\n",
    "    device = torch.device(\"cuda\") # device = GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # device = CPU\n",
    "\n",
    "network.to(device) # Copy model to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "First, some terminology. \n",
    "\n",
    "An **epoch** is a single forward and backward pass through ALL of the samples.\n",
    "\n",
    "A **batch**, then, refers to some subset of an the total dataset, where `batch_size` is the number of samples used in one forward and backward pass.\n",
    "\n",
    "The **number of iterations** is the number of passes (forward and backward $\\implies$ 1 pass) needed to complete a single epoch with each pass using `batch_size` number of samples.\n",
    "\n",
    "In other words, suppose that we have 50,000 samples and `batch_size=25`, then there are 50,000/25 == 2000 iterations for 1 epoch.\n",
    "\n",
    "TODO: Explain backpropagation at high level.\n",
    "\n",
    "TODO: Add practical explanation to code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-20-fc2bcf27bc13>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-fc2bcf27bc13>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def train(network, optimizer=optimizer, loss_fn, train_loader, val_loader,\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def train(network, loss_fn, train_loader, val_loader,\n",
    "          n_epochs, optimizer=optimizer, device=device):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad() # clears gradient buffers of all parameters\n",
    "            inputs, target = batch\n",
    "            # transfer batch data to computation device\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            output = network(inputs)\n",
    "            loss = loss_fn(output, target)\n",
    "            # back propagation\n",
    "            loss.backward()\n",
    "            optimizer.step() # update model weights\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(train_iterator)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- <a id='s0'> </a> UCI Machine Learning Repository. *Facebook Comment Volume Dataset*. https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset# \n",
    "- <a id='s1'></a>Goodfellow, I., Bengio, Y., Courville, A. (2016). *Deep learning* (Vol. 1). Cambridge: MIT press.\n",
    "- Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). *The expressive power of neural networks: A view from the width*. In Advances in neural information processing systems (pp. 6231-6239).\n",
    "- Brownlee, J. (2019). A gentle introduction to the rectified linear unit (relu). *Machine Learning Mastery. https://machinelearningmastery.com/rectified-linear-activation-function-fordeep-learning-neural-networks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Next Steps:\n",
    "### (Computer Vision) Cat Dog\n",
    "  - Save a CNN and use it on this dataset. \n",
    "  - Explain fundamental CNN concepts. \n",
    "  - Data from [Kaggle competition](https://www.kaggle.com/c/dogs-vs-cats/overview)\n",
    "  - [jaeboklee](https://www.kaggle.com/jaeboklee/pytorch-cat-vs-dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shapes\n",
      "Training set:(40949, 54), Testing set:(10044, 54)\n",
      "Dataset shapes after PCA and random sampling\n",
      "X_train.shape:(10237, 10), Y_train.shape:(10237,)\n",
      "X_test.shape:(5022, 53), Y_test.shape:(5022,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uniqu\\anaconda3\\envs\\ds_env\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:302: RuntimeWarning: invalid value encountered in true_divide\n",
      "  corr /= X_norms\n",
      "C:\\Users\\uniqu\\anaconda3\\envs\\ds_env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "C:\\Users\\uniqu\\anaconda3\\envs\\ds_env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "C:\\Users\\uniqu\\anaconda3\\envs\\ds_env\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n"
     ]
    }
   ],
   "source": [
    "exec(open(\"datasets/FB_comments/process_data.py\").read())\n",
    "[A.shape for A in [X_train, Y_train, X_test, Y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10237, 10), (10237,), (5022, 53), (5022,)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
