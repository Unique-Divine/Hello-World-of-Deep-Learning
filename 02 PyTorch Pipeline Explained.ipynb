{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Data-Loading\" data-toc-modified-id=\"Imports-and-Data-Loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Data Loading</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataLoader-class\" data-toc-modified-id=\"DataLoader-class-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><a href=\"https://pytorch.org/docs/stable/data.html\" target=\"_blank\">DataLoader class</a></a></span></li></ul></li><li><span><a href=\"#Building-a-Neural--Network\" data-toc-modified-id=\"Building-a-Neural--Network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Building a Neural  Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Activation Functions</a></span></li><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Loss Function</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimization\" data-toc-modified-id=\"Optimization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Optimization</a></span></li><li><span><a href=\"#GPU\" data-toc-modified-id=\"GPU-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>GPU</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Training Loop</a></span></li></ul></li><li><span><a href=\"#Predictions\" data-toc-modified-id=\"Predictions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Predictions</a></span></li><li><span><a href=\"#Saving-and-Loading-the-Model\" data-toc-modified-id=\"Saving-and-Loading-the-Model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Saving and Loading the Model</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>References</a></span></li><li><span><a href=\"#Next-Steps:\" data-toc-modified-id=\"Next-Steps:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Next Steps:</a></span><ul class=\"toc-item\"><li><span><a href=\"#(Computer-Vision)-Cat-Dog\" data-toc-modified-id=\"(Computer-Vision)-Cat-Dog-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>(Computer Vision) Cat Dog</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "\n",
    "# standard DS stack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "# embed static images in the ipynb\n",
    "%matplotlib inline \n",
    "\n",
    "# neural network package\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "# computer vision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# dataset loading\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "\n",
    "import copy\n",
    "# import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DataLoader class](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "Every dataset, no matter whether what it includes, can interact with PyTorch if it satisfies the following abstract Python class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" Retrieve an item from the dataset in a (label, tensor) pair.\n",
    "        \n",
    "        Args:\n",
    "            idx: index\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the size of the dataset (len)\"\"\"\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is referred to as a map-style dataset in the docs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural  Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module): # class inherits from nn.Module\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # initialize nn.Module\n",
    "        # some fully connected layers w/ linear transformation\n",
    "        \"\"\" nn.Linear(in_features, out_features, bias=True)\n",
    "        Args:\n",
    "            in_features: size of each input sample. For input shape (28, 28), \n",
    "                we would have in_features = 28 * 28 = 784\n",
    "            out_features: size of each output sample.\n",
    "        \"\"\"\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "    def forward(self, x): # defines the forward propagation\n",
    "        x = F.relu(self.fc1(x)) # relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        # Output layer needs a multiclassifying transformation\n",
    "        # log softmax works for this\n",
    "        return F.log_softmax(x, dim=1)\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above network is known as a **feedforward neural network** or **multilayer perception (MLP)**. It's called feedforward because there are no **feedback** connections in which the outputs of the model are fed back into previous layers.  \n",
    "\n",
    "When feedforward neural networks are extended to include feedback connections, they are called **recurrent neural netowrks**. \n",
    "\n",
    "The **depth** of a network is defined as its number of layers (including the output layer but excluding input layer), while the **width** of a network is defined to be the maximal number of nodes in a layer. This explains the reasoning behind the name, \"deep learning\".\n",
    "\n",
    "The terminology for the network structure above is typical called the **network architecture**, which includes how many layers the network contains, how the layers are connected to each other, and how many neurons (a.k.a nodes, a.k.a. units) are in each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions are used to compute hidden layer values.\n",
    "\n",
    "Here, we make use of the **rectified linear unit (ReLU)** as the activation function, using `F.relu`. This is the default recommendation for the activation function in modern deep learning. Here's [an article](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) about ReLU for your reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Explain loss function. \n",
    "\n",
    "Training a network involves passing data through the network, using the loss function to ~~determine~~ define a criterion for capturing the similarity or difference between a prediction and an actual target.\n",
    "\n",
    "TODO: Implement loss function\n",
    "\n",
    "TODO: Explain choice of loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "The information about the loss function that is gained when we pass data through the network is used to update the weights of the network such that we  minimize the loss function.\n",
    "\n",
    "In order to perform the updates on the neural network, we use an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Adam optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **learning rate**, `lr`, is often a key parameter that needs to be tweaked in order to get a network to learn properly and efficiently.\n",
    "\n",
    "Adaptive moment estimation (Adam) and stochastic gradient descent (SGD) have been empirically shown to outperform most other optimizers in deep learning networks. \n",
    "\n",
    "I decided to use Adam for this tutorial because it, along with RMSProp and AdaGrad, uses an adaptive learning rate, which adapts its updates to each paramter depending on the importance of individual paramters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "PyTorch, by default, does CPU-based calculations. To take advantage of the GPU, the input tensors and model need to be moved to the GPU explicitly with the `to()` method.\n",
    "\n",
    "`network` is simply an instance of the neural network class written above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=3, bias=True)\n",
       "  (fc3): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU Recipe:\n",
    "if torch.cuda.is_available(): # If PyTorch reports that GPU is available\n",
    "    device = torch.device(\"cuda\") # device = GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # device = CPU\n",
    "\n",
    "network = net\n",
    "network.to(device) # Copy model to device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "TODO: Explain backpropagation at high level.\n",
    "\n",
    "TODO: Add practical explanation to code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, optimizer=optimizer, loss_fn, train_loader, val_loader,\n",
    "          n_epochs, device=device):\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, target = batch\n",
    "            # transfer batch data to \n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            output = network(inputs)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item()\n",
    "        training_losss /= len(train_iterator)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Goodfellow, I., Bengio, Y., Courville, A., & Bengio, Y. (2016). *Deep learning* (Vol. 1). Cambridge: MIT press.\n",
    "- Lu, Z., Pu, H., Wang, F., Hu, Z., & Wang, L. (2017). *The expressive power of neural networks: A view from the width*. In Advances in neural information processing systems (pp. 6231-6239).\n",
    "- Brownlee, J. (2019). A gentle introduction to the rectified linear unit (relu). *Machine Learning Mastery. https://machinelearningmastery.com/rectified-linear-activation-function-fordeep-learning-neural-networks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Next Steps:\n",
    "### (Computer Vision) Cat Dog\n",
    "  - Save a CNN and use it on this dataset. \n",
    "  - Explain fundamental CNN concepts. \n",
    "  - Data from [Kaggle competition](https://www.kaggle.com/c/dogs-vs-cats/overview)\n",
    "  - [jaeboklee](https://www.kaggle.com/jaeboklee/pytorch-cat-vs-dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
